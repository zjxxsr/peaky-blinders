回答框架：总分总

神经元是什么？ 

```
总：神经元是神经网络的基本构建块，他们负责数据的处理和信息传递，神经元主要在全连接里面使用，用于对特征的提取，识别加分类
分：神经元的组成部分是线性层套个激活函数，公式是WX+B
```

训练的是什么，目的得到什么东西？

```
训练的是参数，也就是w,b
得到最优的w,b,loss下降到最小的时候，最小值左右震荡，说明训练可以了，可以保存参数了。
```

神经元高级回答！

```
1.神经元是什么？
2.适用于什么场景？
3.神经元里面的结构和原理
4.神经元的优缺点?
```

激活函数用在哪？干啥用的？都有哪些？

```
https://blog.csdn.net/BGoodHabit/article/details/106217527?spm=1001.2014.3001.5506
激活函数作用：让线性变成非线性，目的是更好的拟合多项式
sigmoid tanh  softmax relu leakrelu gelu
```

前馈神经网络

```
https://blog.csdn.net/rongsenmeng2835/article/details/108571335?spm=1001.2014.3001.5506
输入层，隐藏层，输出层
输入层：one-hot(好理解)  word2vc（那一个人的数据来举例子）
word2vc 有两个训练方式和两个加速方式
cbow:周围词预测中心词
skip gram：中心词预测周围词
加速方式：
层次softmax:把一个多分类问题转化成多个二分类问题，时间复杂度从0(n)变为0(logn)
负采样：把一个多分类问题转化成一个二分类问题，构造负采样，中心词和周围词构造一个正样本，随机和表中六七个词组成负样本，大大提升速度
隐藏层：靠全连接层工作（激活函数tanh）
输出层：softmax
```

时间复杂度，空间复杂度

```

```

word embeding

```
https://zhuanlan.zhihu.com/p/419804103
```

什么是张量，什么是向量

```
張量：[1,2,3,4,5]
向量[[1,2,3,4,5,6]]
```

激活函数

```
https://blog.csdn.net/BGoodHabit/article/details/106217527?spm=1001.2014.3001.5506
```

什么是求导？

```
求极限值，求斜率
```

dropout作用？

```
让神经元进行一定比例的失活，从而让剩下的神经元努力工作，更好的识别特征，让模型更好
```

CNN卷积神经网络？

```
https://blog.csdn.net/u013069552/article/details/108722192?spm=1001.2014.3001.5506
cnn  卷积(提取特征 卷积核 (3*3 5*5) ，激活（线性变非线性,为了更好的拟合多项式）， 池化(最大池化 平均池化。降维度，除去冗余特征，提纯， 池化核(2*2 4*4))， 全连接（做特征提取 分类） ，softmax 最终概率输出 
```

RNN，LSTM ，GRU ？

```
LSTM:
输入门，遗忘门，输出门(Sigmoid)
C(记忆细胞)H(隐藏状态)
```

什么是梯度下降？反向传播？

```
https://blog.csdn.net/weixin_40857506/article/details/121009503?spm=1001.2014.3001.5506
https://blog.csdn.net/ft_sunshine/article/details/90221691?spm=1001.2014.3001.5506 反向传播
https://blog.csdn.net/qq_41800366/article/details/86583789?spm=1001.2014.3001.5506 梯度下降
```

优化器：

```
 https://blog.csdn.net/caip12999203000/article/details/127455203?spm=1001.2014.3001.5506
 优化器是用来求取模型的最优解的
 优化器可以优化损失函数,以使损失函数最小化的方式更改可训练参数,损失函数指导优化器朝正确的方向移动。

```

![image-20231115100856556](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231115100856556.png)



   GPT系列： https://mp.weixin.qq.com/s/nXazdaf2g048SbDKYb--aw

   transformer https://blog.csdn.net/Tink1995/article/details/105080033?spm=1001.2014.3001.5506

   bert: https://blog.csdn.net/sophicchen/article/details/117804862?spm=1001.2014.3001.5506

   https://blog.csdn.net/liujian20150808/article/details/105215963?spm=1001.2014.3001.5506

   预训练模型汇总：https://blog.csdn.net/xixiaoyaoww/article/details/105460319?spm=1001.2014.3001.5506

transformer

![image-20231115102014622](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231115102014622.png)

input embedding  词嵌入

```
one-hot GloVe Word2ve
```

positional embedding  位置嵌入 位置编码

```
sin #偶数  cos  #奇数
```

Multi-Head attention

![image-20231115110739646](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231115110739646.png)

layer_norm   batch_norm

![image-20231115141148778](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231115141148778.png)

```
Bert：
1：作用：编码器 输入一句话 输出一个向量作为整句话的语义表达   
2：原理和结构：
	a:两个训练任务：MLM 多分类（15% 80% 10% 10%） NSP二分类（判断2句话是不是连续） 
    b: 损失函数2部分组成 loss = loss（mlm)+ loss(NSP) 越小越好 ,交叉熵损失函数 
    c:模型结构：
    	输入：3个embedding 相加  
    	拼接方式： cls  第一句话  sep 第二句话 sep  注意：wordpiece  apple (ap #ple) 解决oov未登录词问题  token 最大长度512 包括（cls sep)
		模型结构： N-BLOCK   多头注意力机制，残差网络，层次归一化，前馈神经网络（2层）
			多头注意力机制：多头作用-- 一词多意，让一个词在不通的语义场景可以很好的表达。 
			自注意力机制：Q=K=V 一句话内部 学习词与词之间相互关系 （内部学习）
            做的步骤：1 分头，线性映射 多组QKV  举例：8个头 512/8 =64  每个头是 64维度
            		2 进入不通QKV语义场景学习  softmax(q*k/根号dk)*v   
            			q*k作用：计算词之间的相似度，相关性 。
            			除以根号DK: 防止q*k结果分布在 SOFTMAX 的饱和区 （0 或者1 附近）
            			softmax 作用： 归一化 0-1区间
            			乘以v 的作用： V保留了原始矩阵维度，更新v里面的参数
            		3 合并 按照最后一个维度 拼接 还原最终的输入维度
            		4 每一步的维度变换： 2个词 每个词64维度 ---  q*k = 2*64  * 64* 2 = 2* 2  
            			q*k 除以根号dk 维度不变 2*2      softmax之后 维度不变  2*2
            			乘以v   2*2  *  2*64  = 2*64  里面的参数都进行加权，加入注意力机制在里面
            残差网络：防止训练的时候 梯度消失 。做法：输入加到多头注意立即的输出上 
            层次归一化：和批次归一化的区别     归一化公式： 每个样本点-均值/标准差  
            		层次和批次归一化的区别：批次归一化--不同样本在同一维度做归一化，层次归一化：同一样本不同维度做归一化
            		意义：单独考虑一个维度是没有意义的，综合考虑所有维度有意义
            前馈神经网络：2层  768-1024  1024-768
            
        d: 优缺点： 优点：效果好，不需要大量语料，只需要少量的语料做微调就会得到很好的效果
        		  缺点： 1：模型大线上用不了，推理速度慢，需要做蒸馏或者使用蒸馏模型
        		  		2：如果是一篇文章 或者长文本 处理不了 ，输入限制 解决方案：滑动窗口编码，取平均当作最终的语义表达
        		  		3：训练的时候使用Mask 但微调的时候看不到Mask 标识， mlm 有10%的词不变就是缓解这个问题
        wordpiece作用： 为了解决oov问题 : out of vocabulary	 未登录词	
        BPE 
        		  
				

transformer：
1：机器翻译，文本摘要，文本生成 都可以用这个框架
2：包括Encoder 和 Decoder 两大部分，
	Encoder(6个BLOCK)  包括： encoder 输入 ，encoder的子层结构
	Decoder（6个BLOCK) 包括： decoder 输入，decoder子层结构 ，decoder的输出
		encoder输入包括：词嵌入 + 位置嵌入
		encoder 子层结构包括：多头注意力机制，残差网络，层次归一化，前馈神经网络
		
		decoder 输入包括： 词嵌入+位置嵌入
		decoder 子层结构包括：mask多头注意力机制，残差网络，层次归一化， encoder-decoder交叉注意力机制，前馈神经网络	
		decoder 输出： 线性层+softmax (词表大小分类)
			
		 	encoder多头注意力机制 ： 指的是Q=K=V  作用：让一个词在不同的语义表达更丰富， softmax(q*kT / 根号dk) * V
			残差网络：防止梯度消失
			层次归一化：在多个维度进行归一化可以让模型效果更好，批次归一化单独考虑一个维度归一化没有任何意义。
			前馈神经网络：2层的网络（2层的线性层  512-1024  1024-512）
			
			decoder mask多头注意力机制: 为什么用mask：训练的时候语料全给模型，测试的时候decoder是一个字一个字生成，当预测										第N个字的时候 是看不到N+1之后的词的信息的，而训练的时候是可以看到后面的词的信息的，									 为了保持训练和预测方式一致，用mask形式来做，用0来遮盖，把第N个词后面的都遮蔽掉
			
			encoder-decoder交叉注意力机制： Q ！= K = V , Q是decoder 提供的， K,V 是encoder的输出
			
			训练的时候，数据是一次性全部给模型，预测的时候只给encoder的数据，decoder数据是不给的，
			decoder 生成的时候是从 start 其实符号开始，当碰到end符号 结束。从左都右 一个字一个字生成
			
gpt系列: https://mp.weixin.qq.com/s/nXazdaf2g048SbDKYb--aw

```

chatgpt原理? 参考李宏毅的讲解

1. 使用无监督语料训练一个GPT语言模型，让模型有基础的语言回复能力

2. 有监督的微调 SFT ,  人类老师进场  【promt="世界最大的山峰是什么？"，answer:"珠穆朗玛峰"】

3. RLHF (基于人类反馈的强化学习)    使用一条prompt   再使用已经训练好的模型 生成4条答案，并进行人类打分，然后降序排列使用强化学习的方式进行训练。

4. 使用PPO策略更新参数

   ```
   1：使用无监督语料训练一个GPT语言模型，让模型有基础的语言回复能力
   文字接龙：珠穆朗玛峰
   世界最大的山峰是什么？
   世界最大的山峰是什么？--珠
   世界最大的山峰是什么？珠--穆
   世界最大的山峰是什么？珠穆--朗
   世界最大的山峰是什么？珠穆朗--玛
   世界最大的山峰是什么？珠穆朗玛--峰
   
   2： 有监督的微调 SFT
   【promt="世界最大的山峰是什么？"，answer:"珠穆朗玛峰"】 语料形式  问答对---人工挑选的有用的问答对，让模型学会进行优质回复，人类想要的回复给学会了
   
   3： RLHF (基于人类反馈的强化学习)  
   使用一个prompt 扔进训练好的模型中生成3-4个答案
   
   我穿这件衣服好看吗？prompt   ---- 好看                      2
   我穿这件衣服好看吗？prompt   ---- 你穿这件衣服好显白啊         3
   我穿这件衣服好看吗？prompt   ---- 你就是街上最亮的崽           1
   我穿这件衣服好看吗？prompt   ---- 今天我看到天使了            4       
   
   4 3 2 1 
   
   奖励模型---GPT 6b 
   问题和回复扔进 奖励模型 去得到分数 
   
   PPO 训练策略
   ```

   ```
   介绍一下PPO，它的策略优化是怎么做的
   答案：
   PPO （proximal policy optimization，PPO）近端策略优化，核心思想是限制策略更新幅度，以达到稳定、高效的训练结果。
   PPO 算法之所以被提出，根本原因在于 Policy Gradient（策略梯度） 在处理连续动作空间时 Learning rate 取值抉择困难。Learning rate 取值过小，就会导致深度强化学习收敛性较差，陷入完不成训练的局面，取值过大则导致新旧策略迭代时数据不一致，造成学习波动较大或局部震荡。除此之外，Policy Gradient 因为在线学习的性质，进行迭代策略时原先的采样数据无法被重复利用，每次迭代都需要重新采样。
   具体来说，PPO算法使用了两个损失函数：第一个损失函数是近端比率裁剪损失，用于限制策略更新幅度；第二个损失函数是价值函数损失，用于优化策略。两个损失函数的加权和就是PPO算法的总损失函数。
   ```

   neo4j
   
   ```
   https://blog.csdn.net/sinat_37574187/article/details/125641593?spm=1001.2014.3001.5506
   ```
   
   结构化数据：关系数据库、链接数据
   
   半结构化数据：XML、JSON、百科
   
   非结构化数据：图片、音频、视频

​      

