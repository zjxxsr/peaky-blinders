--------------------------------0--------------------------------
--------------------------------0:train--------------------------------
batch before to dev: {'labels': tensor([3, 0, 4, 2, 3, 2, 3, 3, 2, 4, 0, 4, 3, 0, 1, 1, 3, 0, 4, 3, 4, 2, 1, 3,
        3, 0, 1, 1, 3, 0, 4, 1]), 'input_ids': tensor([[  101,   109,   127,  ...,     0,     0,     0],
        [  101,  1706,  1660,  ..., 14452,  1964,   102],
        [  101,   146,  5608,  ...,     0,     0,     0],
        ...,
        [  101,  8768,  1164,  ...,     0,     0,     0],
        [  101,  3477,  7925,  ...,     0,     0,     0],
        [  101,  1220,  1912,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]])}
type of batch: <class 'dict'>
shape of input_ids, token_type_ids, attention_mask: torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 512])
batch after to dev: {'labels': tensor([3, 0, 4, 2, 3, 2, 3, 3, 2, 4, 0, 4, 3, 0, 1, 1, 3, 0, 4, 3, 4, 2, 1, 3,
        3, 0, 1, 1, 3, 0, 4, 1], device='cuda:0'), 'input_ids': tensor([[  101,   109,   127,  ...,     0,     0,     0],
        [  101,  1706,  1660,  ..., 14452,  1964,   102],
        [  101,   146,  5608,  ...,     0,     0,     0],
        ...,
        [  101,  8768,  1164,  ...,     0,     0,     0],
        [  101,  3477,  7925,  ...,     0,     0,     0],
        [  101,  1220,  1912,  ...,     0,     0,     0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}
type of batch: <class 'dict'>
shape of input_ids, token_type_ids, attention_mask: torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 512])
outputs: SequenceClassifierOutput(loss=tensor(1.7657, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1943, -0.3972,  0.6350, -0.5853, -0.8264],
        [ 0.5300, -0.1854,  0.6074, -0.0212, -0.3461],
        [ 0.2692, -0.5048,  0.8194, -0.4839, -0.6587],
        [ 0.3698, -0.5400,  0.6115, -0.5089, -0.5723],
        [ 0.4271, -0.6406,  0.5913, -0.4190, -0.6368],
        [ 0.2580, -0.7294,  0.7287, -0.5983, -0.5645],
        [ 0.5108, -0.3372,  0.5068, -0.2687, -0.2759],
        [ 0.5502, -0.2882,  0.7823, -0.2767, -0.4442],
        [ 0.5336, -0.4020,  0.6112, -0.2480, -0.4820],
        [ 0.4500, -0.6163,  0.6243, -0.6432, -0.3892],
        [ 0.4338, -0.4107,  0.8867, -0.2233, -0.5733],
        [ 0.2622, -0.3574,  0.4525, -0.1494, -0.4647],
        [ 0.4516, -0.5150,  0.4628, -0.4399, -0.8805],
        [ 0.7085, -0.2525,  0.5184, -0.2982, -0.4723],
        [ 0.3144, -0.3124,  0.6341, -0.2451, -0.4247],
        [ 0.4458, -0.1635,  0.7307, -0.4345, -0.3968],
        [ 0.2833, -0.3765,  0.7488, -0.2824, -0.3250],
        [ 0.5317, -0.3164,  0.9931, -0.5903, -0.6716],
        [ 0.5619, -0.1395,  0.5170, -0.2076, -0.4486],
        [ 0.3876, -0.3797,  0.3256,  0.1026, -0.3418],
        [ 0.3520,  0.1357,  0.6413, -0.1454, -0.4556],
        [ 0.3046, -0.3865,  1.0018, -0.3197, -0.4188],
        [ 0.3819, -0.1627,  0.7578, -0.4414, -0.6164],
        [ 0.7364, -0.5843,  0.8649, -0.4514, -0.4617],
        [ 0.3177, -0.5012,  0.8422, -0.1444, -0.4277],
        [ 0.3812, -0.4821,  0.5563, -0.7128, -0.6493],
        [ 0.3781, -0.5371,  0.5794, -0.3404, -0.7503],
        [ 0.3293, -0.5779,  0.3009, -0.6488, -0.3694],
        [ 0.5058, -0.4783,  0.5867, -0.4177, -0.6668],
        [ 0.3876,  0.0680,  0.2852, -0.3305, -0.0455],
        [ 0.4440, -0.3676,  0.7356, -0.6339, -0.6291],
        [ 0.3964, -0.1110,  0.5435, -0.3128, -0.3455]], device='cuda:0',
       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
type of outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>
outputs.logits.shape: torch.Size([32, 5])
loss: tensor(1.7657, device='cuda:0', grad_fn=<NllLossBackward0>)
type of loss: <class 'torch.Tensor'>
#epoch0-0: before: loss=1.7657179832458496 lr=5e-05 lr=[5e-05]
#epoch0-0: after: loss=1.7657179832458496 lr=4.99e-05 lr=[4.99e-05]
#epoch0-1: before: loss=1.6563469171524048 lr=4.99e-05 lr=[4.99e-05]
#epoch0-1: after: loss=1.6563469171524048 lr=4.9800000000000004e-05 lr=[4.9800000000000004e-05]
#epoch0-2: before: loss=1.6084439754486084 lr=4.9800000000000004e-05 lr=[4.9800000000000004e-05]
#epoch0-2: after: loss=1.6084439754486084 lr=4.97e-05 lr=[4.97e-05]
#epoch0-3: before: loss=1.5949068069458008 lr=4.97e-05 lr=[4.97e-05]
#epoch0-3: after: loss=1.5949068069458008 lr=4.96e-05 lr=[4.96e-05]
#epoch0-4: before: loss=1.6245219707489014 lr=4.96e-05 lr=[4.96e-05]
#epoch0-4: after: loss=1.6245219707489014 lr=4.9500000000000004e-05 lr=[4.9500000000000004e-05]
#epoch0-5: before: loss=1.5946612358093262 lr=4.9500000000000004e-05 lr=[4.9500000000000004e-05]
#epoch0-5: after: loss=1.5946612358093262 lr=4.94e-05 lr=[4.94e-05]
#epoch0-6: before: loss=1.630807876586914 lr=4.94e-05 lr=[4.94e-05]
#epoch0-6: after: loss=1.630807876586914 lr=4.93e-05 lr=[4.93e-05]
#epoch0-7: before: loss=1.6245088577270508 lr=4.93e-05 lr=[4.93e-05]
#epoch0-7: after: loss=1.6245088577270508 lr=4.92e-05 lr=[4.92e-05]
#epoch0-8: before: loss=1.5653306245803833 lr=4.92e-05 lr=[4.92e-05]
#epoch0-8: after: loss=1.5653306245803833 lr=4.91e-05 lr=[4.91e-05]
#epoch0-9: before: loss=1.6056004762649536 lr=4.91e-05 lr=[4.91e-05]
#epoch0-9: after: loss=1.6056004762649536 lr=4.9e-05 lr=[4.9e-05]
#epoch0-10: before: loss=1.5402154922485352 lr=4.9e-05 lr=[4.9e-05]
#epoch0-10: after: loss=1.5402154922485352 lr=4.89e-05 lr=[4.89e-05]
#epoch0-11: before: loss=1.6225172281265259 lr=4.89e-05 lr=[4.89e-05]
#epoch0-11: after: loss=1.6225172281265259 lr=4.88e-05 lr=[4.88e-05]
#epoch0-12: before: loss=1.6664694547653198 lr=4.88e-05 lr=[4.88e-05]
#epoch0-12: after: loss=1.6664694547653198 lr=4.87e-05 lr=[4.87e-05]
#epoch0-13: before: loss=1.7479918003082275 lr=4.87e-05 lr=[4.87e-05]
#epoch0-13: after: loss=1.7479918003082275 lr=4.86e-05 lr=[4.86e-05]
#epoch0-14: before: loss=1.6380106210708618 lr=4.86e-05 lr=[4.86e-05]
#epoch0-14: after: loss=1.6380106210708618 lr=4.85e-05 lr=[4.85e-05]
#epoch0-15: before: loss=1.6447422504425049 lr=4.85e-05 lr=[4.85e-05]
#epoch0-15: after: loss=1.6447422504425049 lr=4.8400000000000004e-05 lr=[4.8400000000000004e-05]
#epoch0-16: before: loss=1.6610963344573975 lr=4.8400000000000004e-05 lr=[4.8400000000000004e-05]
#epoch0-16: after: loss=1.6610963344573975 lr=4.83e-05 lr=[4.83e-05]
#epoch0-17: before: loss=1.5913881063461304 lr=4.83e-05 lr=[4.83e-05]
#epoch0-17: after: loss=1.5913881063461304 lr=4.82e-05 lr=[4.82e-05]
#epoch0-18: before: loss=1.6540627479553223 lr=4.82e-05 lr=[4.82e-05]
#epoch0-18: after: loss=1.6540627479553223 lr=4.8100000000000004e-05 lr=[4.8100000000000004e-05]
#epoch0-19: before: loss=1.6401751041412354 lr=4.8100000000000004e-05 lr=[4.8100000000000004e-05]
#epoch0-19: after: loss=1.6401751041412354 lr=4.8e-05 lr=[4.8e-05]
#epoch0-20: before: loss=1.5799425840377808 lr=4.8e-05 lr=[4.8e-05]
#epoch0-20: after: loss=1.5799425840377808 lr=4.79e-05 lr=[4.79e-05]
#epoch0-21: before: loss=1.6775569915771484 lr=4.79e-05 lr=[4.79e-05]
#epoch0-21: after: loss=1.6775569915771484 lr=4.78e-05 lr=[4.78e-05]
#epoch0-22: before: loss=1.680742621421814 lr=4.78e-05 lr=[4.78e-05]
#epoch0-22: after: loss=1.680742621421814 lr=4.77e-05 lr=[4.77e-05]
#epoch0-23: before: loss=1.632053017616272 lr=4.77e-05 lr=[4.77e-05]
#epoch0-23: after: loss=1.632053017616272 lr=4.76e-05 lr=[4.76e-05]
#epoch0-24: before: loss=1.6015957593917847 lr=4.76e-05 lr=[4.76e-05]
#epoch0-24: after: loss=1.6015957593917847 lr=4.75e-05 lr=[4.75e-05]
#epoch0-25: before: loss=1.5714360475540161 lr=4.75e-05 lr=[4.75e-05]
#epoch0-25: after: loss=1.5714360475540161 lr=4.74e-05 lr=[4.74e-05]
#epoch0-26: before: loss=1.665349006652832 lr=4.74e-05 lr=[4.74e-05]
#epoch0-26: after: loss=1.665349006652832 lr=4.73e-05 lr=[4.73e-05]
#epoch0-27: before: loss=1.6139816045761108 lr=4.73e-05 lr=[4.73e-05]
#epoch0-27: after: loss=1.6139816045761108 lr=4.72e-05 lr=[4.72e-05]
#epoch0-28: before: loss=1.5890980958938599 lr=4.72e-05 lr=[4.72e-05]
#epoch0-28: after: loss=1.5890980958938599 lr=4.71e-05 lr=[4.71e-05]
#epoch0-29: before: loss=1.557269811630249 lr=4.71e-05 lr=[4.71e-05]
#epoch0-29: after: loss=1.557269811630249 lr=4.7e-05 lr=[4.7e-05]
#epoch0-30: before: loss=1.6323797702789307 lr=4.7e-05 lr=[4.7e-05]
#epoch0-30: after: loss=1.6323797702789307 lr=4.69e-05 lr=[4.69e-05]
#epoch0-31: before: loss=1.5578694343566895 lr=4.69e-05 lr=[4.69e-05]
#epoch0-31: after: loss=1.5578694343566895 lr=4.6800000000000006e-05 lr=[4.6800000000000006e-05]
#epoch0-32: before: loss=1.621822476387024 lr=4.6800000000000006e-05 lr=[4.6800000000000006e-05]
#epoch0-32: after: loss=1.621822476387024 lr=4.6700000000000003e-05 lr=[4.6700000000000003e-05]
#epoch0-33: before: loss=1.6137923002243042 lr=4.6700000000000003e-05 lr=[4.6700000000000003e-05]
#epoch0-33: after: loss=1.6137923002243042 lr=4.660000000000001e-05 lr=[4.660000000000001e-05]
#epoch0-34: before: loss=1.5390046834945679 lr=4.660000000000001e-05 lr=[4.660000000000001e-05]
#epoch0-34: after: loss=1.5390046834945679 lr=4.6500000000000005e-05 lr=[4.6500000000000005e-05]
#epoch0-35: before: loss=1.5937069654464722 lr=4.6500000000000005e-05 lr=[4.6500000000000005e-05]
#epoch0-35: after: loss=1.5937069654464722 lr=4.64e-05 lr=[4.64e-05]
#epoch0-36: before: loss=1.6631410121917725 lr=4.64e-05 lr=[4.64e-05]
#epoch0-36: after: loss=1.6631410121917725 lr=4.630000000000001e-05 lr=[4.630000000000001e-05]
#epoch0-37: before: loss=1.598220705986023 lr=4.630000000000001e-05 lr=[4.630000000000001e-05]
#epoch0-37: after: loss=1.598220705986023 lr=4.6200000000000005e-05 lr=[4.6200000000000005e-05]
#epoch0-38: before: loss=1.620774269104004 lr=4.6200000000000005e-05 lr=[4.6200000000000005e-05]
#epoch0-38: after: loss=1.620774269104004 lr=4.61e-05 lr=[4.61e-05]
#epoch0-39: before: loss=1.5833085775375366 lr=4.61e-05 lr=[4.61e-05]
#epoch0-39: after: loss=1.5833085775375366 lr=4.600000000000001e-05 lr=[4.600000000000001e-05]
#epoch0-40: before: loss=1.5766839981079102 lr=4.600000000000001e-05 lr=[4.600000000000001e-05]
#epoch0-40: after: loss=1.5766839981079102 lr=4.5900000000000004e-05 lr=[4.5900000000000004e-05]
#epoch0-41: before: loss=1.6251819133758545 lr=4.5900000000000004e-05 lr=[4.5900000000000004e-05]
#epoch0-41: after: loss=1.6251819133758545 lr=4.58e-05 lr=[4.58e-05]
#epoch0-42: before: loss=1.6528902053833008 lr=4.58e-05 lr=[4.58e-05]
#epoch0-42: after: loss=1.6528902053833008 lr=4.5700000000000006e-05 lr=[4.5700000000000006e-05]
#epoch0-43: before: loss=1.6256991624832153 lr=4.5700000000000006e-05 lr=[4.5700000000000006e-05]
#epoch0-43: after: loss=1.6256991624832153 lr=4.5600000000000004e-05 lr=[4.5600000000000004e-05]
#epoch0-44: before: loss=1.6057389974594116 lr=4.5600000000000004e-05 lr=[4.5600000000000004e-05]
#epoch0-44: after: loss=1.6057389974594116 lr=4.55e-05 lr=[4.55e-05]
#epoch0-45: before: loss=1.604010820388794 lr=4.55e-05 lr=[4.55e-05]
#epoch0-45: after: loss=1.604010820388794 lr=4.5400000000000006e-05 lr=[4.5400000000000006e-05]
#epoch0-46: before: loss=1.636256217956543 lr=4.5400000000000006e-05 lr=[4.5400000000000006e-05]
#epoch0-46: after: loss=1.636256217956543 lr=4.53e-05 lr=[4.53e-05]
#epoch0-47: before: loss=1.6233512163162231 lr=4.53e-05 lr=[4.53e-05]
#epoch0-47: after: loss=1.6233512163162231 lr=4.52e-05 lr=[4.52e-05]
#epoch0-48: before: loss=1.5525658130645752 lr=4.52e-05 lr=[4.52e-05]
#epoch0-48: after: loss=1.5525658130645752 lr=4.5100000000000005e-05 lr=[4.5100000000000005e-05]
#epoch0-49: before: loss=1.647292137145996 lr=4.5100000000000005e-05 lr=[4.5100000000000005e-05]
#epoch0-49: after: loss=1.647292137145996 lr=4.5e-05 lr=[4.5e-05]
#epoch0-50: before: loss=1.571718454360962 lr=4.5e-05 lr=[4.5e-05]
#epoch0-50: after: loss=1.571718454360962 lr=4.49e-05 lr=[4.49e-05]
#epoch0-51: before: loss=1.6259452104568481 lr=4.49e-05 lr=[4.49e-05]
#epoch0-51: after: loss=1.6259452104568481 lr=4.4800000000000005e-05 lr=[4.4800000000000005e-05]
#epoch0-52: before: loss=1.5894200801849365 lr=4.4800000000000005e-05 lr=[4.4800000000000005e-05]
#epoch0-52: after: loss=1.5894200801849365 lr=4.47e-05 lr=[4.47e-05]
#epoch0-53: before: loss=1.6405078172683716 lr=4.47e-05 lr=[4.47e-05]
#epoch0-53: after: loss=1.6405078172683716 lr=4.46e-05 lr=[4.46e-05]
#epoch0-54: before: loss=1.5410010814666748 lr=4.46e-05 lr=[4.46e-05]
#epoch0-54: after: loss=1.5410010814666748 lr=4.4500000000000004e-05 lr=[4.4500000000000004e-05]
#epoch0-55: before: loss=1.5929707288742065 lr=4.4500000000000004e-05 lr=[4.4500000000000004e-05]
#epoch0-55: after: loss=1.5929707288742065 lr=4.44e-05 lr=[4.44e-05]
#epoch0-56: before: loss=1.5466376543045044 lr=4.44e-05 lr=[4.44e-05]
#epoch0-56: after: loss=1.5466376543045044 lr=4.43e-05 lr=[4.43e-05]
#epoch0-57: before: loss=1.55435311794281 lr=4.43e-05 lr=[4.43e-05]
#epoch0-57: after: loss=1.55435311794281 lr=4.4200000000000004e-05 lr=[4.4200000000000004e-05]
#epoch0-58: before: loss=1.670985460281372 lr=4.4200000000000004e-05 lr=[4.4200000000000004e-05]
#epoch0-58: after: loss=1.670985460281372 lr=4.41e-05 lr=[4.41e-05]
#epoch0-59: before: loss=1.585573673248291 lr=4.41e-05 lr=[4.41e-05]
#epoch0-59: after: loss=1.585573673248291 lr=4.4000000000000006e-05 lr=[4.4000000000000006e-05]
#epoch0-60: before: loss=1.5760424137115479 lr=4.4000000000000006e-05 lr=[4.4000000000000006e-05]
#epoch0-60: after: loss=1.5760424137115479 lr=4.39e-05 lr=[4.39e-05]
#epoch0-61: before: loss=1.5590852499008179 lr=4.39e-05 lr=[4.39e-05]
#epoch0-61: after: loss=1.5590852499008179 lr=4.38e-05 lr=[4.38e-05]
#epoch0-62: before: loss=1.5937000513076782 lr=4.38e-05 lr=[4.38e-05]
#epoch0-62: after: loss=1.5937000513076782 lr=4.3700000000000005e-05 lr=[4.3700000000000005e-05]
#epoch0-63: before: loss=1.8036366701126099 lr=4.3700000000000005e-05 lr=[4.3700000000000005e-05]
#epoch0-63: after: loss=1.8036366701126099 lr=4.36e-05 lr=[4.36e-05]
#epoch0-64: before: loss=1.5635883808135986 lr=4.36e-05 lr=[4.36e-05]
#epoch0-64: after: loss=1.5635883808135986 lr=4.35e-05 lr=[4.35e-05]
#epoch0-65: before: loss=1.650962471961975 lr=4.35e-05 lr=[4.35e-05]
#epoch0-65: after: loss=1.650962471961975 lr=4.3400000000000005e-05 lr=[4.3400000000000005e-05]
#epoch0-66: before: loss=1.4933743476867676 lr=4.3400000000000005e-05 lr=[4.3400000000000005e-05]
#epoch0-66: after: loss=1.4933743476867676 lr=4.33e-05 lr=[4.33e-05]
#epoch0-67: before: loss=1.6587281227111816 lr=4.33e-05 lr=[4.33e-05]
#epoch0-67: after: loss=1.6587281227111816 lr=4.32e-05 lr=[4.32e-05]
#epoch0-68: before: loss=1.620298981666565 lr=4.32e-05 lr=[4.32e-05]
#epoch0-68: after: loss=1.620298981666565 lr=4.3100000000000004e-05 lr=[4.3100000000000004e-05]
#epoch0-69: before: loss=1.524139642715454 lr=4.3100000000000004e-05 lr=[4.3100000000000004e-05]
#epoch0-69: after: loss=1.524139642715454 lr=4.3e-05 lr=[4.3e-05]
#epoch0-70: before: loss=1.5750799179077148 lr=4.3e-05 lr=[4.3e-05]
#epoch0-70: after: loss=1.5750799179077148 lr=4.29e-05 lr=[4.29e-05]
#epoch0-71: before: loss=1.5871844291687012 lr=4.29e-05 lr=[4.29e-05]
#epoch0-71: after: loss=1.5871844291687012 lr=4.2800000000000004e-05 lr=[4.2800000000000004e-05]
#epoch0-72: before: loss=1.5723332166671753 lr=4.2800000000000004e-05 lr=[4.2800000000000004e-05]
#epoch0-72: after: loss=1.5723332166671753 lr=4.27e-05 lr=[4.27e-05]
#epoch0-73: before: loss=1.55790376663208 lr=4.27e-05 lr=[4.27e-05]
#epoch0-73: after: loss=1.55790376663208 lr=4.26e-05 lr=[4.26e-05]
#epoch0-74: before: loss=1.6065748929977417 lr=4.26e-05 lr=[4.26e-05]
#epoch0-74: after: loss=1.6065748929977417 lr=4.25e-05 lr=[4.25e-05]
#epoch0-75: before: loss=1.6024885177612305 lr=4.25e-05 lr=[4.25e-05]
#epoch0-75: after: loss=1.6024885177612305 lr=4.24e-05 lr=[4.24e-05]
#epoch0-76: before: loss=1.5319185256958008 lr=4.24e-05 lr=[4.24e-05]
#epoch0-76: after: loss=1.5319185256958008 lr=4.23e-05 lr=[4.23e-05]
#epoch0-77: before: loss=1.523397445678711 lr=4.23e-05 lr=[4.23e-05]
#epoch0-77: after: loss=1.523397445678711 lr=4.22e-05 lr=[4.22e-05]
#epoch0-78: before: loss=1.573163390159607 lr=4.22e-05 lr=[4.22e-05]
#epoch0-78: after: loss=1.573163390159607 lr=4.21e-05 lr=[4.21e-05]
#epoch0-79: before: loss=1.527155876159668 lr=4.21e-05 lr=[4.21e-05]
#epoch0-79: after: loss=1.527155876159668 lr=4.2e-05 lr=[4.2e-05]
#epoch0-80: before: loss=1.5224601030349731 lr=4.2e-05 lr=[4.2e-05]
#epoch0-80: after: loss=1.5224601030349731 lr=4.19e-05 lr=[4.19e-05]
#epoch0-81: before: loss=1.606358528137207 lr=4.19e-05 lr=[4.19e-05]
#epoch0-81: after: loss=1.606358528137207 lr=4.18e-05 lr=[4.18e-05]
#epoch0-82: before: loss=1.6348249912261963 lr=4.18e-05 lr=[4.18e-05]
#epoch0-82: after: loss=1.6348249912261963 lr=4.17e-05 lr=[4.17e-05]
#epoch0-83: before: loss=1.609015941619873 lr=4.17e-05 lr=[4.17e-05]
#epoch0-83: after: loss=1.609015941619873 lr=4.16e-05 lr=[4.16e-05]
#epoch0-84: before: loss=1.604426622390747 lr=4.16e-05 lr=[4.16e-05]
#epoch0-84: after: loss=1.604426622390747 lr=4.15e-05 lr=[4.15e-05]
#epoch0-85: before: loss=1.5378077030181885 lr=4.15e-05 lr=[4.15e-05]
#epoch0-85: after: loss=1.5378077030181885 lr=4.14e-05 lr=[4.14e-05]
#epoch0-86: before: loss=1.5574198961257935 lr=4.14e-05 lr=[4.14e-05]
#epoch0-86: after: loss=1.5574198961257935 lr=4.13e-05 lr=[4.13e-05]
#epoch0-87: before: loss=1.4708280563354492 lr=4.13e-05 lr=[4.13e-05]
#epoch0-87: after: loss=1.4708280563354492 lr=4.12e-05 lr=[4.12e-05]
#epoch0-88: before: loss=1.5863054990768433 lr=4.12e-05 lr=[4.12e-05]
#epoch0-88: after: loss=1.5863054990768433 lr=4.11e-05 lr=[4.11e-05]
#epoch0-89: before: loss=1.611611008644104 lr=4.11e-05 lr=[4.11e-05]
#epoch0-89: after: loss=1.611611008644104 lr=4.1e-05 lr=[4.1e-05]
#epoch0-90: before: loss=1.52413809299469 lr=4.1e-05 lr=[4.1e-05]
#epoch0-90: after: loss=1.52413809299469 lr=4.09e-05 lr=[4.09e-05]
#epoch0-91: before: loss=1.5518388748168945 lr=4.09e-05 lr=[4.09e-05]
#epoch0-91: after: loss=1.5518388748168945 lr=4.08e-05 lr=[4.08e-05]
#epoch0-92: before: loss=1.46760094165802 lr=4.08e-05 lr=[4.08e-05]
#epoch0-92: after: loss=1.46760094165802 lr=4.07e-05 lr=[4.07e-05]
#epoch0-93: before: loss=1.4099931716918945 lr=4.07e-05 lr=[4.07e-05]
#epoch0-93: after: loss=1.4099931716918945 lr=4.0600000000000004e-05 lr=[4.0600000000000004e-05]
#epoch0-94: before: loss=1.5358943939208984 lr=4.0600000000000004e-05 lr=[4.0600000000000004e-05]
#epoch0-94: after: loss=1.5358943939208984 lr=4.05e-05 lr=[4.05e-05]
#epoch0-95: before: loss=1.4546005725860596 lr=4.05e-05 lr=[4.05e-05]
#epoch0-95: after: loss=1.4546005725860596 lr=4.0400000000000006e-05 lr=[4.0400000000000006e-05]
#epoch0-96: before: loss=1.600866675376892 lr=4.0400000000000006e-05 lr=[4.0400000000000006e-05]
#epoch0-96: after: loss=1.600866675376892 lr=4.0300000000000004e-05 lr=[4.0300000000000004e-05]
#epoch0-97: before: loss=1.493355393409729 lr=4.0300000000000004e-05 lr=[4.0300000000000004e-05]
#epoch0-97: after: loss=1.493355393409729 lr=4.02e-05 lr=[4.02e-05]
#epoch0-98: before: loss=1.4295588731765747 lr=4.02e-05 lr=[4.02e-05]
#epoch0-98: after: loss=1.4295588731765747 lr=4.0100000000000006e-05 lr=[4.0100000000000006e-05]
#epoch0-99: before: loss=1.2525181770324707 lr=4.0100000000000006e-05 lr=[4.0100000000000006e-05]
#epoch0-99: after: loss=1.2525181770324707 lr=4e-05 lr=[4e-05]
#epoch0-100: before: loss=1.4982355833053589 lr=4e-05 lr=[4e-05]
#epoch0-100: after: loss=1.4982355833053589 lr=3.99e-05 lr=[3.99e-05]
#epoch0-101: before: loss=1.528108835220337 lr=3.99e-05 lr=[3.99e-05]
#epoch0-101: after: loss=1.528108835220337 lr=3.9800000000000005e-05 lr=[3.9800000000000005e-05]
#epoch0-102: before: loss=1.357636570930481 lr=3.9800000000000005e-05 lr=[3.9800000000000005e-05]
#epoch0-102: after: loss=1.357636570930481 lr=3.97e-05 lr=[3.97e-05]
#epoch0-103: before: loss=1.5304315090179443 lr=3.97e-05 lr=[3.97e-05]
#epoch0-103: after: loss=1.5304315090179443 lr=3.960000000000001e-05 lr=[3.960000000000001e-05]
#epoch0-104: before: loss=1.4975005388259888 lr=3.960000000000001e-05 lr=[3.960000000000001e-05]
#epoch0-104: after: loss=1.4975005388259888 lr=3.9500000000000005e-05 lr=[3.9500000000000005e-05]
#epoch0-105: before: loss=1.587095022201538 lr=3.9500000000000005e-05 lr=[3.9500000000000005e-05]
#epoch0-105: after: loss=1.587095022201538 lr=3.94e-05 lr=[3.94e-05]
#epoch0-106: before: loss=1.4883997440338135 lr=3.94e-05 lr=[3.94e-05]
#epoch0-106: after: loss=1.4883997440338135 lr=3.9300000000000007e-05 lr=[3.9300000000000007e-05]
#epoch0-107: before: loss=1.48500394821167 lr=3.9300000000000007e-05 lr=[3.9300000000000007e-05]
#epoch0-107: after: loss=1.48500394821167 lr=3.9200000000000004e-05 lr=[3.9200000000000004e-05]
#epoch0-108: before: loss=1.5436280965805054 lr=3.9200000000000004e-05 lr=[3.9200000000000004e-05]
#epoch0-108: after: loss=1.5436280965805054 lr=3.91e-05 lr=[3.91e-05]
#epoch0-109: before: loss=1.405734658241272 lr=3.91e-05 lr=[3.91e-05]
#epoch0-109: after: loss=1.405734658241272 lr=3.9000000000000006e-05 lr=[3.9000000000000006e-05]
#epoch0-110: before: loss=1.4018932580947876 lr=3.9000000000000006e-05 lr=[3.9000000000000006e-05]
#epoch0-110: after: loss=1.4018932580947876 lr=3.8900000000000004e-05 lr=[3.8900000000000004e-05]
#epoch0-111: before: loss=1.4147539138793945 lr=3.8900000000000004e-05 lr=[3.8900000000000004e-05]
#epoch0-111: after: loss=1.4147539138793945 lr=3.88e-05 lr=[3.88e-05]
#epoch0-112: before: loss=1.4224687814712524 lr=3.88e-05 lr=[3.88e-05]
#epoch0-112: after: loss=1.4224687814712524 lr=3.8700000000000006e-05 lr=[3.8700000000000006e-05]
#epoch0-113: before: loss=1.355468988418579 lr=3.8700000000000006e-05 lr=[3.8700000000000006e-05]
#epoch0-113: after: loss=1.355468988418579 lr=3.86e-05 lr=[3.86e-05]
#epoch0-114: before: loss=1.421374797821045 lr=3.86e-05 lr=[3.86e-05]
#epoch0-114: after: loss=1.421374797821045 lr=3.85e-05 lr=[3.85e-05]
#epoch0-115: before: loss=1.2872587442398071 lr=3.85e-05 lr=[3.85e-05]
#epoch0-115: after: loss=1.2872587442398071 lr=3.8400000000000005e-05 lr=[3.8400000000000005e-05]
#epoch0-116: before: loss=1.4612090587615967 lr=3.8400000000000005e-05 lr=[3.8400000000000005e-05]
#epoch0-116: after: loss=1.4612090587615967 lr=3.83e-05 lr=[3.83e-05]
#epoch0-117: before: loss=1.3219398260116577 lr=3.83e-05 lr=[3.83e-05]
#epoch0-117: after: loss=1.3219398260116577 lr=3.82e-05 lr=[3.82e-05]
#epoch0-118: before: loss=1.6438822746276855 lr=3.82e-05 lr=[3.82e-05]
#epoch0-118: after: loss=1.6438822746276855 lr=3.8100000000000005e-05 lr=[3.8100000000000005e-05]
#epoch0-119: before: loss=1.526463270187378 lr=3.8100000000000005e-05 lr=[3.8100000000000005e-05]
#epoch0-119: after: loss=1.526463270187378 lr=3.8e-05 lr=[3.8e-05]
#epoch0-120: before: loss=1.3724409341812134 lr=3.8e-05 lr=[3.8e-05]
#epoch0-120: after: loss=1.3724409341812134 lr=3.79e-05 lr=[3.79e-05]
#epoch0-121: before: loss=1.4006998538970947 lr=3.79e-05 lr=[3.79e-05]
#epoch0-121: after: loss=1.4006998538970947 lr=3.7800000000000004e-05 lr=[3.7800000000000004e-05]
#epoch0-122: before: loss=1.2797837257385254 lr=3.7800000000000004e-05 lr=[3.7800000000000004e-05]
#epoch0-122: after: loss=1.2797837257385254 lr=3.77e-05 lr=[3.77e-05]
#epoch0-123: before: loss=1.2900408506393433 lr=3.77e-05 lr=[3.77e-05]
#epoch0-123: after: loss=1.2900408506393433 lr=3.76e-05 lr=[3.76e-05]
#epoch0-124: before: loss=1.4416357278823853 lr=3.76e-05 lr=[3.76e-05]
#epoch0-124: after: loss=1.4416357278823853 lr=3.7500000000000003e-05 lr=[3.7500000000000003e-05]
#epoch0: loss=1.4416357278823853 lr=3.7500000000000003e-05 lr=[3.7500000000000003e-05]
--------------------------------saving to /root/autodl-tmp/jupyter_notebook.large.d/native_bert-base-cased_yelp_review_full_2023_10_01_12_23_12_878232_temp1_4000/epoch-0--------------------------------
--------------------------------saved--------------------------------
--------------------------------0:evaluate--------------------------------
outputs: SequenceClassifierOutput(loss=tensor(1.4712, device='cuda:0'), logits=tensor([[ 1.3190,  0.7918, -0.1167, -1.1633, -1.6870],
        [-0.6595, -0.1973, -0.2734,  0.8335,  1.2024],
        [-0.7872, -0.4310, -0.0348,  0.9150,  1.2303],
        [ 0.2027,  0.4182,  0.1712, -0.5487, -1.0103],
        [-0.7003, -0.2473, -0.2259,  0.8472,  1.2089],
        [ 1.6393,  0.8011, -0.2681, -1.1242, -1.5865],
        [-0.7149, -0.2780, -0.1879,  0.8649,  1.2073],
        [-0.6212, -0.1575, -0.3102,  0.8093,  1.1980],
        [-0.6447, -0.2201, -0.2987,  0.8252,  1.2301],
        [-0.6828, -0.2521, -0.2611,  0.8441,  1.2334],
        [ 1.2228,  0.6391, -0.1013, -0.9822, -1.4890],
        [-0.6523, -0.2364, -0.3032,  0.8214,  1.2475],
        [ 0.0701, -0.1545,  0.1998,  0.1737,  0.4996],
        [-0.7510, -0.3323, -0.1328,  0.8914,  1.2186],
        [-0.6419, -0.2103, -0.3039,  0.8174,  1.2356],
        [-0.6679, -0.1995, -0.2632,  0.8310,  1.1862],
        [-0.7902, -0.6108,  0.3138,  0.8946,  1.2376],
        [ 0.9458,  0.6172,  0.0244, -0.8968, -1.3282],
        [-0.8054, -0.5472,  0.0415,  0.9214,  1.2506],
        [-0.6424, -0.5434,  0.2047,  0.7892,  1.2637],
        [-0.7012, -0.3425, -0.2884,  0.8457,  1.2990],
        [-0.6566, -0.2212, -0.2999,  0.8246,  1.2414],
        [-0.8011, -0.5162,  0.0300,  0.9383,  1.2377],
        [-0.6727, -0.1978, -0.2707,  0.8420,  1.1925],
        [-0.6658, -0.1972, -0.2744,  0.8329,  1.1999],
        [ 0.2733,  0.5389,  0.1759, -0.2770, -0.9500],
        [-0.6515, -0.2221, -0.2945,  0.8263,  1.2283],
        [-0.6289, -0.1622, -0.3004,  0.8141,  1.1942],
        [-0.6496, -0.2201, -0.3031,  0.8226,  1.2269],
        [-0.2374, -0.2858,  0.3122,  0.3995,  0.4407],
        [-0.7872, -0.6180,  0.2055,  0.9396,  1.2535],
        [-0.6555, -0.2029, -0.2702,  0.8339,  1.1972]], device='cuda:0'), hidden_states=None, attentions=None)
type of outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>
outputs.logits.shape: torch.Size([32, 5])
logits shape: torch.Size([32, 5])
val_loss = 1.4887524843215942
val_acc = {'accuracy': 0.33975}
--------------------------------1--------------------------------
--------------------------------1:train--------------------------------
#epoch1: loss=1.077115774154663 lr=2.5e-05 lr=[2.5e-05]
--------------------------------saving to /root/autodl-tmp/jupyter_notebook.large.d/native_bert-base-cased_yelp_review_full_2023_10_01_12_23_12_878232_temp1_4000/epoch-1--------------------------------
--------------------------------saved--------------------------------
--------------------------------1:evaluate--------------------------------
val_loss = 1.0705550909042358
val_acc = {'accuracy': 0.53875}
--------------------------------2--------------------------------
--------------------------------2:train--------------------------------
#epoch2: loss=0.7102513909339905 lr=1.25e-05 lr=[1.25e-05]
--------------------------------saving to /root/autodl-tmp/jupyter_notebook.large.d/native_bert-base-cased_yelp_review_full_2023_10_01_12_23_12_878232_temp1_4000/epoch-2--------------------------------
--------------------------------saved--------------------------------
--------------------------------2:evaluate--------------------------------
val_loss = 0.9755904674530029
val_acc = {'accuracy': 0.58}
--------------------------------3--------------------------------
--------------------------------3:train--------------------------------
#epoch3: loss=0.6994549632072449 lr=0.0 lr=[0.0]
--------------------------------saving to /root/autodl-tmp/jupyter_notebook.large.d/native_bert-base-cased_yelp_review_full_2023_10_01_12_23_12_878232_temp1_4000/epoch-3--------------------------------
--------------------------------saved--------------------------------
--------------------------------3:evaluate--------------------------------
val_loss = 1.092982530593872
val_acc = {'accuracy': 0.56}
--------------------------------All over--------------------------------